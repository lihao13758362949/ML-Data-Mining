# 0 写在前面
参考资料：
 1. 《机器学习》周志华

# 1 贝叶斯决策论
对于给定的样本$x$和需要被正确标记的$c$，要得到使$P(c|x)$最大的贝叶斯最优分类器。
基于贝叶斯公式，有
$$P(c|x)=\frac{P(x|c)P(c)}{P(x)},$$
其中，$P(c)$是"先验概率"，根据大数定律可由训练集中标记出现的频率来估计；

$P(x|c)$是似然（likelihood），由于它涉及关于$x$所有属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难。例如,假设样本的$d$个属性都是二值的，则样本空间将有$2^d$种可能的取值，在现实应用中，这个值往往远大于训练样本数$m$,也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计$P(x |c)$显然不可行，因为“未被观测到”与“出现概率为零”通常是不同的。**估计的方法见后面的章节；**

$P(x)$与标记无关。
# 2 参数估计方法
## 2.1 极大似然估计（Maximum Likelihood Estimation,MLE）
频率主义学派认为参数虽然未知，但却是客观存在的固定值，因此可以通过优化似然函数等准则来确定参数值。
假设$P(x|c)$有确定的形式并且被参数向量$\bf{\theta}_c$唯一确定，将$P(x|c)$记为$P(x|{\bf \theta_c})$。
$D_c$表示训练集D中第c类样本组成的集合，若样本独立同分布则
$$P(D_c|\theta_c)=\prod_{x\in D_c}P(x|\theta_c),$$
故
$$L(\theta_c)=lnP(D_c|\theta_c)=\sum_{x\in D_c}lnP(x|\theta_c),$$
此时$\theta_c$极大似然估计为$\hat \theta_c=\argmax_{\theta_c}L(\theta_c)$。
显然，这种估计方法需要提前知道所给样本的分布或总体的分布，这在实际应用中难以实施。
## 2.2 朴素贝叶斯分类器
贝叶斯学派认为参数本身也有分布，因此可假定参数服从一个先验分布（一般为均匀分布），然后基于样本来计算后验分布。
若**假设各个属性之间独立**，则$\displaystyle P(c|x)=\frac{P(x|c)P(c)}{P(x)}=\frac{P(c)}{P(x)}\prod _{i=1}^dP(x_i|c),$由于每个类别的$P(x)$相同，则可以得到朴素贝叶斯分类器的表达式
$$h_{n b}(\boldsymbol{x})=\underset{c \in \mathcal{Y}}{\arg \max } P(c) \prod_{i=1}^{d} P\left(x_{i} | c\right),$$
其中d为属性的个数，$x_i$为x在第i个属性上的取值，$P(c)=\frac{|D_c|}{|D|}$（假设样本独立同分布），$P(x_i|c)=\frac{|D_{c,x_i}|}{|D_c|}$（此为离散形式，连续属性采用概率密度$p\left(x_{i} | c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_{i}-\mu_{c, i}\right)^{2}}{2 \sigma_{c, i}^{2}}\right)$）。
### 2.2.1 修正
显然若出现没有在训练集中出现的属性值和类别，$h_{nb}(x)=P(x_i|c)=0$则无论该样本的其它属性是什么，都认为是负类别样本。故修正为
$$\hat{P}(c)=\frac{\left|D_{c}\right|+1}{|D|+N},\hat{P}\left(x_{i} | c\right)=\frac{\left|D_{c, x_{i}}\right|+1}{\left|D_{c}\right|+N_{i}},$$其中，$N_i$表示第$i$个属性可能取值的个数。
## 2.3 半朴素贝叶斯分类器
朴素贝叶斯分类器的前提假设十分苛刻，故半朴素贝叶斯分类器对该条件进行一定程度的放松。半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算,又不至于彻底忽略了比较强的属性依赖关系。
### 2.3.1 独依赖估计(One-Dependent Estimator,简称ODE)
独依赖估计是半朴素贝叶斯分类器最常用的一种策略。顾名思议，所谓“独依赖”就是假设每个属性在类别之外最多仅依赖于一个其他属性。
SPODE
### 2.3.2 TAN
### 2.3.3 AODE
## 2.4 贝叶斯网
## 2.5 EM算法
