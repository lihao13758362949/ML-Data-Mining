1. https://zhuanlan.zhihu.com/p/54192512
优化算法有很多种，如果按梯度的类型进行分类，可以分为有梯度优化算法和无梯度优化算法，有梯度优化算法主要有梯度下降法、动量法momentum、Adagrad、RMSProp、Adadelta、Adam等，无梯度优化算法也有很多，像粒子群优化算法、蚁群算法群体智能优化算法，也有贝叶斯优化、ES、SMAC这一类的黑盒优化算法，这篇文章主要介绍一下有梯度优化算法。
https://pic1.zhimg.com/80/v2-c726db927d17180330071c52d6f541a8_1440w.jpg


梯度下降法:梯度下降使用整个训练数据集来计算梯度
随机梯度下降:如果使用梯度下降法(批量梯度下降法)，那么每次迭代过程中都要对 m 个样本进行求梯度，所以开销非常大，随机梯度下降的思想就是随机采样一个样本来更新参数，那么计算开销就从 O(m)下降到 O(1) 。
小批量梯度下降法:选取一定数目的样本组成一个小批量样本，然后用这个小批量更新梯度，这样不仅可以减少计算成本，还可以提高算法稳定性

动量法momentum：动量法在梯度下降法的基础上结合指数加权平均的思想加入一个动量变量 vt来控制不同方向的梯度，使得各个方向的梯度移动一致，

Adagrad:Adagrad算法是根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。
RMSProp、Adadelta、Adam
